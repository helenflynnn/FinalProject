---
title: "Tidying Public Water Quality Data"
author: "Helen Flynn"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc:  yes
    toc_depth:  3
    toc_float: true
editor_options: 
  chunk_output_type: console
---

# Why public datasets?

Working with large, open-access data sets can serve many purposes. It can be an excellent way to explore new ideas, before investing in field-work or experiments. It can be a great way to take local or experimental results and expand them to different ecosystems, places, or landscapes. Or it can be an excellent way to build, validate, and test ecological models on regional or national scales. 

So why doesn't everyone use public data? Well, it's often collected by a variety of organizations, with different methods, units, and inconsistent metadata. Together these issues make large public data sets "messy." Messy data can be messy in many different ways, but at the basic level it means that data is hard to analyze; not because the data itself is bad, but because the way it is organized is unclear or inconsistent. 

In this lab, we will learn some tricks to "tidying" data, making it analysis-ready. We will depend heavily on the [tidyverse](https://www.tidyverse.org/), an excellent series of packages that make data manipulation beautiful and easy. We will also be working with water quality portal data so we will also use the excellent [dataRetrieval](https://github.com/USGS-R/dataRetrieval) package for downloading data from the Water Quality Portal and the USGS. 

## Loading key packages

This lesson is meant to introduce the incredible variety of tools that one can use to clean data, many of these tools are captured by the `tidyverse` meta-package, a package of packages, but there are some additional ones that will help us locate our various water quality sites. 

```{r setup, warnings='hide',message=FALSE}
library(tidyverse) #Package with dplyr, tibble, readr, and others to help clean coding
library(dataRetrieval) #Package to download data. 
library(sf) #Geospatial package to plot and explore data
library(mapview) #Simple interface to leaflet interactive maps
library(broom) #Simplifies model outputs
library(knitr) #Makes nice tables
library(kableExtra) #Makes even nicer tables
library(lubridate) #Makes working with dates easier
library(ggthemes) #Makes plots prettier
library(tidyr) #Makes multiple simultaneous models easier

#Move the directory to the top folder level
#knitr::opts_knit$set(root.dir='..')
```

# Downloading data

For this lab, we'll explore water quality data in the Colorado River basin as it moves from Colorado to Arizona. All data will be generated through the code you see below, with the only external information coming from knowing the SiteID's for the monitoring locations along the Colorado River and the water quality characteristic names. 

The water quality portal can be accessed with the command `readWQPdata`, which takes a variety of parameters (like startdate, enddate, constituents, etc...). We'll generate these rules for downloading the data here. 

## Download prep

```{r download-prep}
# First we'll make a tibble (a tidyverse table) with Site IDs. Generally these are increasingly downstream of the CO headwaters near Grand Lake. 
colorado <- tibble(sites=c('USGS-09034500', 'USGS-09069000',
                           'USGS-09085000', 'USGS-09095500', 'USGS-09152500'),
                   basin=c('colorado1', 'eagle',
                           'roaring', 'colorado3', 'gunnison'))

# Now we need to setup a series of rules for downloading data from the Water Quality Portal. 
# We'll focus on cation and anion data from 1950-present. Each cation has a name that we might 
# typically use like calcium or sulfate, but the name may be different in the water quality
# portal, so we have to check this website https://www.waterqualitydata.us/Codes/Characteristicname?mimeType=xml 
# to get our names correct. 

paramater.names <- c('ca', 'mg', 'na', 'k', 'so4', 'cl', 'hco3')

ca <- 'Calcium'
mg <- 'Magnesium'
na <- 'Sodium'
k <- 'Potassium'
so4 <- c('Sulfate', 'Sulfate as SO4', 'Sulfur Sulfate', 'Total Sulfate')
cl <- 'Chloride'
hco3 <- c('Alkalinity, bicarbonate', 'Bicarbonate')

# Compile all these names into a single list
parameters <- list(ca, mg, na, k, so4, cl, hco3)

# Name each cation or anion in the list
names(parameters) <- paramater.names

# Notice that we aren't downloading any nutrients (P or N) because they are much messier (100s of different ways to measure and report concentration 
# data) than other cation anion data. 

# Start dates
start <- '1980-10-01'
end <- '2023-01-01'

# Sample media (no sediment samples)
sampleMedia = 'Water'

# Compile all this information into a list with arguments
site.args <- list(siteid = colorado$sites,
                  sampleMedia = sampleMedia,
                  startDateLo = start,
                  startDateHi = end,
                  characteristicName = NA) # We'll fill this in later in a loop
```

## Concentration data download

Now that we have generated the commands to download the data, the code to download the data is here, but it is not run on purpose because it takes 15 minutes or so to run every time. You can always run it yourself by setting `eval = T`. 

```{r concentration download}
conc.list <- list() # Empty list to hold each data download

# We'll loop over each anion or cation and download all data at our sites for that constituent
for(i in 1:length(parameters)){
  
  # We need to rename the characteristicName (constituent) each time we go through the loop
  site.args$characteristicName <- parameters[[i]]
  
  # readWQPdata takes in our site.args list and downloads the data according to those rules 
  # time, constituent, site, etc...
  
  # Don't forget about pipes "%>%"! Pipes pass forward the results of a previous command, so that 
  # you don't have to constantly rename variables. I love them. 
  
  conc.list[[i]] <- readWQPdata(site.args) %>%
    mutate(parameter = names(parameters)[i]) #Mutate just adds a new column to the data frame
  
  # Pipes make the above command simple and succinct versus something more complicated like:
  # conc.list[[i]] <- readWQPdata(site.args)
  # conc.list[[i]]$parameter <- names(parameters)[i]

}

conc.long <- conc.list %>% bind_rows()
```

# Data tidying

Now that we have downloaded the data, we need to tidy it up. The water quality portal data comes with an incredible amount of metadata in the form of extra columns, but we don't need all this extra data. 

Look at the data you downloaded:

```{r conc data}
head(conc.long) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width = '800px',height = '300px')

```

## Initial cleaning up

Wow, that looks messy! Lots of extraneous columns, lots of NAs, so much information we can hardly parse it. Let's pare it down to the essentials. 

```{r tidying up concentration}
# This code mostly just grabs and renames the most important data columns
conc.clean <-  conc.long %>%
  dplyr::select(date = ActivityStartDate,
                parameter = CharacteristicName,
                units = ResultMeasure.MeasureUnitCode,
                SiteID = MonitoringLocationIdentifier,
                org = OrganizationFormalName,
                org_id = OrganizationIdentifier,
                time = ActivityStartTime.Time,
                value = ResultMeasureValue,
                sample_method = SampleCollectionMethod.MethodName,
                analytical_method = ResultAnalyticalMethod.MethodName,
                particle_size = ResultParticleSizeBasisText,
                date_time = ActivityStartDateTime,
                media = ActivityMediaName,
                sample_depth = ActivityDepthHeightMeasure.MeasureValue,
                sample_depth_unit = ActivityDepthHeightMeasure.MeasureUnitCode,
                fraction = ResultSampleFractionText,
                status = ResultStatusIdentifier) %>%
  # Remove trailing white space in labels
  mutate(units  =  trimws(units)) %>%
  # Keep only samples that are water samples
  filter(media == 'Water') 

```

Now let's look at the tidier version:

```{r examine tidier data}
head(conc.clean) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width = '800px', height = '300px')
```

## Final tidy dataset

Okay, that is getting better, but we still have lots of extraneous information. For our purposes, let's assume that the sample and analytical methods used by the USGS are reasonable and exchangeable (one method is equivalent to the other). If we make that assumption then the only remaining tidying step left is to make sure that all the data is in the same units. 

### Unit Check

```{r unit check}
table(conc.clean$units)
```

Wow! Almost all the data is in mg/L. That makes our job really easy. 

We just need to remove these observations with a `dplyr::filter()` call and then select an even smaller subset of useful columns, while adding a time object column using the `lubridate::ymd()` call. 

```{r tidy}
conc.tidy <- conc.clean %>% 
  filter(units == 'mg/l') %>%
  # ymd() converts characters into YYYY-MM-DD date formatting:
  mutate(date = lubridate::ymd(date)) %>%
  select(date,
         parameter,
         SiteID,
         conc=value)
```

### Daily data

Now we have a manageable data frame. But how do we want to organize the data? Since we are looking at a really long time-series of data (70 years), let's look at data as a daily average. The `dplyr::group_by()` and `dplyr::summarize()` commands make this really easy:

```{r daily}
# The amazing group_by function groups all the data so that the summary
# only applies to each subgroup (site, date, and parameter combination).
# So in the end you get a daily average concentratino for each site and parameter type. 
conc.daily <- conc.tidy %>%
  group_by(date, parameter, SiteID) %>% 
  summarize(conc = mean(conc, na.rm = T))
```

Taking daily averages looks like it eliminated `r nrow(conc.tidy) - nrow(conc.daily)` observations, meaning these site-date combinations had multiple observations on the same day. 

# Assignment!

Let's imagine you wanted to add data for your water quality analyses, but you also know that you need to do this analysis over and over again. Let's walk through how we would: 1) Add new data to our `conc.clean` data set, and 2) how to write a function to download, clean, and update our data with far less code.

## Question 1.

Write a function that can repeat the above steps with a single function call. This function should take in a single tibble that is identical in structure to the `colorado` one above (e.g. it has columns named `sites`, and `basin`). The function should then take in that tibble and be able to download and clean the data to make the data structure/outcomes exactly like `conc.daily`. Use this function to download data for the three sites listed below. 

```{r}
additional_data <- tibble(sites = c('USGS-09180000', 'USGS-09180500', 'USGS-09380000'),
                          basin = c('dolores', 'colorado4', 'colorado5'))

load_clean <- function(sites, basin){
paramater.names <- c('ca', 'mg', 'na', 'k', 'so4', 'cl', 'hco3')

ca <- 'Calcium'
mg <- 'Magnesium'
na <- 'Sodium'
k <- 'Potassium'
so4 <- c('Sulfate', 'Sulfate as SO4', 'Sulfur Sulfate', 'Total Sulfate')
cl <- 'Chloride'
hco3 <- c('Alkalinity, bicarbonate', 'Bicarbonate')

parameters <- list(ca, mg, na, k, so4, cl, hco3)

names(parameters) <- paramater.names


start <- '1980-10-01'
end <- '2023-01-01'

sampleMedia = 'Water'

site.args <- list(siteid = sites,
                  sampleMedia = sampleMedia,
                  startDateLo = start,
                  startDateHi = end,
                  characteristicName = NA) # We'll fill this in later in a loop
conc.list <- list() 

for(i in 1:length(parameters)){
  
  site.args$characteristicName <- parameters[[i]]
  
  conc.list[[i]] <- readWQPdata(site.args) %>%
    mutate(parameter = names(parameters)[i]) 

}

conc.long <- conc.list %>% bind_rows()

conc.clean <-  conc.long %>%
  dplyr::select(date = ActivityStartDate,
                parameter = CharacteristicName,
                units = ResultMeasure.MeasureUnitCode,
                SiteID = MonitoringLocationIdentifier,
                org = OrganizationFormalName,
                org_id = OrganizationIdentifier,
                time = ActivityStartTime.Time,
                value = ResultMeasureValue,
                sample_method = SampleCollectionMethod.MethodName,
                analytical_method = ResultAnalyticalMethod.MethodName,
                particle_size = ResultParticleSizeBasisText,
                date_time = ActivityStartDateTime,
                media = ActivityMediaName,
                sample_depth = ActivityDepthHeightMeasure.MeasureValue,
                sample_depth_unit = ActivityDepthHeightMeasure.MeasureUnitCode,
                fraction = ResultSampleFractionText,
                status = ResultStatusIdentifier) %>%
  mutate(units  =  trimws(units)) %>%
  filter(media == 'Water') 


conc.tidy <- conc.clean %>% 
  filter(units == 'mg/l') %>%
  mutate(date = lubridate::ymd(date)) %>%
  select(date,
         parameter,
         SiteID,
         conc=value)

conc.daily <- conc.tidy %>%
  group_by(date, parameter, SiteID) %>% 
  summarize(conc = mean(conc, na.rm = T))
}

test <- load_clean(additional_data$sites, additional_data$basin)
```


## Question 2.

Append the new data that the above function returned to `conc.daily` using `bind_rows()`. (Remember, this new data should be identical in structure to the `conc.daily` data set). Save this new data set as `tidied_full_wq.RData` using the `save()` function.

```{r}
tidiedfull <- bind_rows(conc.daily, test)

save(tidiedfull, file = 'data/tidied_full_wq.RData')
```

## Question 3

We now have a dataset of stream water quality data for 9 sites throughout Colorado. However, one potential control on stream chemistry is stream discharge. One function that can allow you to easily download discharge data is `readNWISdv()` from the `dataRetrieval` package. Use this function to download daily discharge data for all eight of the sites you've already worked with above. Save this data as `data/Q.RData`. The site numbers are the same as what we used above, but you need to remove `USGS-` from each site. Reminder, discharge is `00060` for the `parameterCd` argument. Moreover, we can use `renameNWISColumns()` to automatically make the column names a little less annoying.

```{r}
# Reminder! you can use ?readNWISdv to read about how the function works. 
sites <- colorado %>%
  # Bind the two datasets to get all 8 sites
  bind_rows(additional_data) %>%
  # Grab just the column labeled sites
  pull(sites) %>%
  # Remove the USGS- prefix
  gsub('USGS-', '', .)

waterdat <- readNWISdv(siteNumbers = sites, parameterCd = "00060", startDate = "1980-10-01", endDate = "2023-01-01") %>% 
  renameNWISColumns()

save(waterdat, file = 'data/Q.Rdata')
```



